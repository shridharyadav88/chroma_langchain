{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m venv env_LangChain\n",
    "# env_LangChain\\Scripts\\activate.bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import datetime\n",
    "from config import OPENAI_API_KEY\n",
    "import asyncio\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8RxPm8ajR2FaGI8pkygNhaHcYbZbm', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"As an AI language model, I do not have up-to-date information on ChatGPT's latest free version available for use in APIs. It is recommended to check the ChatGPT website or documentation for the latest information on their available versions and API usage.\", role='assistant', function_call=None, tool_calls=None))], created=1701673938, model='gpt-3.5-turbo-0301', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=52, prompt_tokens=25, total_tokens=77))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"Which is the latest free version of ChatGPT which can be used in APIs?\"\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":message\n",
    "        }\n",
    "        ],\n",
    "    model=llm_model\n",
    ")\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI language model, I do not have up-to-date information on ChatGPT's latest free version available for use in APIs. It is recommended to check the ChatGPT website or documentation for the latest information on their available versions and API usage.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.model_dump().get(\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt):\n",
    "    messages = [{\"role\":\"user\", \"message\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model = llm_model\n",
    "    )\n",
    "    return response, response.choices[0].message.model_dump().get(\"content\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"chat_response_object.pkl\",mode='wb') as file:\n",
    "#     pickle.dump(file=file, obj=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./chat_response_object.pkl\", mode=\"rb\") as file: \n",
    "    response_read = pickle.load(file)\n",
    "response_read.choices[0].message.model_dump().get(\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_text = \"\"\"\n",
    "\n",
    "                                                         \n",
    "                   WP(CRL)-2013-132-J       \n",
    "\n",
    "Case No           132 \n",
    "Year              2013 \n",
    "Date of Decision  07/05/2013 \n",
    "Case Type :       WRIT PETITION CRIMINAL \n",
    "Decided By        Hon'ble Mr/Mrs Justice PRATIBHA RANI J \n",
    "Petitioner/      \n",
    "Panes             TINKKU RAM \n",
    "Respondent/ \n",
    "Defendant Name    STATE OF THE NCT OF DELHI \n",
    "Petitioner's/ \n",
    "Plaintiff's Adv   S B DSNDAPANL \n",
    "Respondent's/ \n",
    "Defendant's Adv   SALEEM AHMED \n",
    "Print Date        22-Jul-2015 \n",
    "Company Name      RICOH INDIA LTD. \n",
    "\n",
    "---page_breaker---\n",
    "\n",
    "               W.P (Crl.) 132 2013 \n",
    "\n",
    "Recvived the enclosed jail petition of Sh.Tinku Ram S/o Sh.Saltanti \n",
    "from Central Jail Tihar New Delhi, through Delhi High Court Legal\n",
    "Services Committee, New Delhi vide letter No.215 dated 22.01.2013 \n",
    "\n",
    "Through this jail petition above named petitioner is praying for \n",
    "parole on the ground mentioned in the petition. \n",
    "\n",
    "May the petition for the parole be registered as W.P.(Crl.)  /2013 \n",
    "and be placed before appropriate bench of this court. \n",
    "\n",
    "AOJ/(Crl)\n",
    "\n",
    "IN THE HIGH COURT OF DELHI AT NEW DELHI \n",
    "Filing No. 11719/2013 \n",
    "\n",
    "THE MATTER OF \n",
    "\n",
    "TINKU RAM                        Petitioner \n",
    "                Versus \n",
    "STATE                            Respondent \n",
    "\n",
    "No Connecting matter found. \n",
    "\n",
    "Filing Assistant Kuldeep Kumar \n",
    "\n",
    "\n",
    "AOJ/FILING \n",
    "\"\"\"\n",
    "\n",
    "questions = \"\"\"\n",
    "1: LAWS INVOLVED\n",
    "2: Who is the informant or complainant\n",
    "3: What is the name of the victim\n",
    "4: What is the date or of the commitment of the offence\n",
    "5: What is the place of occurrence of the offence\n",
    "6: How much time was taken by the police to reach the place of occurrence after\n",
    "the receipt of information of commission of the offence\n",
    "7: What is the name of the accused arrested at the spot\n",
    "8: What was the manner of assault\n",
    "9: What is the description of the weapon used during the occurrence of the offence\n",
    "10: What is the description of the specific overt act performed by of the accused\n",
    "person(s)\n",
    "11: Who identified the body of the deceased\n",
    "12: What was the statement of the accused person(s) under section 313 Cr.P.C on\n",
    "examination.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = \"\"\"For the below case file answer these questions. \n",
    "Questions:{questions}\"\n",
    "Case File:{case_text}\n",
    "\n",
    "Format the output as JSON with the question serial number as key and the answer as the value\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['case_text', 'questions'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['case_text', 'questions'], template='For the below case file answer these questions. \\nQuestions:{questions}\"\\nCase File:{case_text}\\n\\nFormat the output as JSON with the question serial number as key and the answer as the value\\n'))]\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shridhar\\Documents\\Shridhar\\Projects\\Langchain\\LangChain.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shridhar/Documents/Shridhar/Projects/Langchain/LangChain.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(prompt_template)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shridhar/Documents/Shridhar/Projects/Langchain/LangChain.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m messages \u001b[39m=\u001b[39m prompt_template\u001b[39m.\u001b[39mformat_messages(questions\u001b[39m=\u001b[39mquestions, case_text\u001b[39m=\u001b[39mcase_text)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shridhar/Documents/Shridhar/Projects/Langchain/LangChain.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m chat \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, model\u001b[39m=\u001b[39;49mllm_model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shridhar/Documents/Shridhar/Projects/Langchain/LangChain.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m response \u001b[39m=\u001b[39m chat(messages)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shridhar/Documents/Shridhar/Projects/Langchain/LangChain.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\Users\\shridhar\\Documents\\Shridhar\\Projects\\Langchain\\env_LangChain\\lib\\site-packages\\langchain_core\\load\\serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\shridhar\\Documents\\Shridhar\\Projects\\Langchain\\env_LangChain\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "print(prompt_template)\n",
    "\n",
    "messages = prompt_template.format_messages(questions=questions, case_text=case_text)\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "response = chat(messages)\n",
    "print(response.content)\n",
    "print(type(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert this output from string format to json format\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
