{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/langchain-ai/langchain/issues/9717\n",
    "# https://python.langchain.com/docs/use_cases/question_answering/\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from config import GOOGLE_AI_API_KEY\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatGooglePalm\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loading\n",
    "path = './case_files./case_file.txt'\n",
    "loader = TextLoader(path)\n",
    "data = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './case_files./case_file.txt'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shridhar\\Documents\\Shridhar\\Projects\\Langchain\\env_LangChain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Create retriever\n",
    "embedding = GooglePalmEmbeddings(google_api_key = GOOGLE_AI_API_KEY)\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to LLM for generation\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the questions at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Keep the answer as concise as possible, Avoid using full sentences for fact related answers.\n",
    "{context}\n",
    "Questions: \n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGooglePalm(google_api_key=GOOGLE_AI_API_KEY, temperature=0.0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "with open('./questions.txt', mode='r') as questions_file:\n",
    "    list_questions = questions_file.readlines()\n",
    "\n",
    "prompt = \"\"\"For the below given context answer these questions. If the information to answer the question is not present in the context say \"I dont know\". \n",
    "Answer as precise and concise as possible.\n",
    "Context: {context}\n",
    "Questions:{questions}\"\n",
    "\"\"\"\n",
    "\n",
    "promt_template_question = ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": vectorstore.as_retriever() | format_docs, \"questions\": RunnablePassthrough()}\n",
    "    | promt_template_question\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_answers = []\n",
    "for question in list_questions:\n",
    "    try:\n",
    "        list_answers.append(rag_chain.invoke(question))\n",
    "    except:\n",
    "        print(\"Exception occured for the question.\", question)\n",
    "        list_answers.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_case = pd.DataFrame(list_questions, list_answers).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_case.to_csv(\"qa_case.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
